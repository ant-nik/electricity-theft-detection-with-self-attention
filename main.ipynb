{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electricity fraud detection based on hybrid attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is based on original paper \"Electricity Theft Detection with self-attention\" by Paulo Finardi, Israel Campiotti, Gustavo Plensack, Rafael Derradi de Souza, Rodrigo Nogueira, Gustavo Pinheiro, Roberto Lotufo (https://arxiv.org/abs/2002.06219). \n",
    "Source code is derived from their repository https://github.com/neuralmind-ai/electricity-theft-detection-with-self-attention.\n",
    "The purpose of the notebook is refactoring and further investigation of the original paper.\n",
    "\n",
    "The notebook organazed as follows. First step is an initialization. It is performed to load all high-level modules that incapsulates low-level code. Second step is a data investigation. Next - training. Final step is a performance metrics calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook contains a data investigation, basic training and performance calculation steps. They are based on high-level modeules that are loaded in this section. For Colab environment source code is cloned at first step (it may be uncommented if it is required). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/ant-nik/electricity-theft-detection-with-self-attention.git\n",
    "#% cd electricity-theft-detection-with-self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you an find high-level description of imported modules.\n",
    "- FraudData object simplifies access to various views of a raw data (raw/normalized, thief/regular, autocorelations etc);\n",
    "- HybridAttentionModel - defines hybrid attention model based on torch library; \n",
    "- perform_kfold_cv - a training routine;\n",
    "- RAdam - training optimizer implementation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.dataset import FraudData, download_data\n",
    "from source.hybrid_attention import HybridAttentionModel\n",
    "from source.train import perform_kfold_cv\n",
    "from source.optimizer import RAdam\n",
    "from torch import nn\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step of initialization is a raw data downloading process is required. It can be done manually or by a routine (it may be issue in Windows if there is no unix utils in OS, so manual loading is more stable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may be issues in Windows environments, so manual downloading may be required.\n",
    "# download_data() # *nix/Colab environments only\n",
    "data = FraudData('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset has a lot of missed values and the authors of the original paper suggested to replace them with zeros and add an additional channel with bitmask that is a signal to classifier that value is mised. It is a good prectice to learn classifiers but according to time series properties such transformation might introduce a distortion to results.\n",
    "\n",
    "Below the raw data is processed in a different way in order to calculate autocorrelation characteristic. It is split in chunks without missed values and only chanks with enough size are processed to calculate autocorrelation (~ to times greter than maximum autocorrelation lag). Autocorrelation results are obtained for four groups of data: thiefs and regular clients data, thiefs and regular clients normalized data. A normalization is performed to stabilize parameters of the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = pyplot.subplots(2,2, figsize=(12,6))\n",
    "vis = [\n",
    "    {'title': \"Thiefs autocorelation\", \n",
    "         'ax': ax1, 'data': data.thief_ac},\n",
    "    {'title': \"Regular client's autocorelation\", \n",
    "         'ax': ax2, 'data': data.regular_ac},\n",
    "    {'title': \"Thief's autocorelation (normalized)\", \n",
    "         'ax': ax3, 'data': data.norm_thief_ac},\n",
    "    {'title': \"Regular client's autocorelation (normalized)\", \n",
    "         'ax': ax4, 'data': data.norm_regular_ac}\n",
    "]\n",
    "for view in vis:\n",
    "    view['ax'].set_title(view['title'])\n",
    "    view['data'].mean(axis=1).plot(ax=ax1, legend=True, style='b-')\n",
    "    view['data'].max(axis=1).plot(ax=ax1, legend=True, style='b--')\n",
    "    view['data'].min(axis=1).plot(ax=ax1, legend=True, style='b--')\n",
    "    view['ax'].legend(['median', 'min/max'])\n",
    "    view['ax'].set_xlabel('lag')\n",
    "    view['ax'].set_ylabel('autocorrelation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Attention model learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of the original paper suggested to use a hybrid classifier that is complex neural network with convolutional and attention layers that forms a basic blocks. A learn process is performed as fold-learning with Adam optimizer and cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- K Fold [1/2] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: [1/1] -- T: 0.347 -- V: 0.301 -- F1: 0.0228\n",
      "AUC: 0.721 -- MAP@100: 0.571 -- MAP@200: 0.526\n",
      "Fold 1 got F1 = 0.0228 at epoch 1\n",
      "Printing report at best checkpoint for F1\n",
      "AUC: 0.721 -- MAP@100: 0.571 -- MAP@200: 0.526\n",
      "--- K Fold [2/2] ---\n",
      "ep: [1/1] -- T: 0.373 -- V: 0.35 -- F1: 0.0175\n",
      "AUC: 0.735 -- MAP@100: 0.613 -- MAP@200: 0.576\n",
      "Fold 2 got F1 = 0.0175 at epoch 1\n",
      "Printing report at best checkpoint for F1\n",
      "AUC: 0.735 -- MAP@100: 0.613 -- MAP@200: 0.576\n"
     ]
    }
   ],
   "source": [
    "k_folds = 5\n",
    "lr = 0.001\n",
    "device = 'cpu'\n",
    "models = [HybridAttentionModel().to(device) for _ in range(k_folds)]\n",
    "optims = [RAdam(model.parameters(), lr) for model in models]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "f1_per_fold = perform_kfold_cv(data.normalized, models, optims, criterion, \n",
    "                               k_folds, device=device, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attention model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of learning are represented as a set of learned model files. They can be loaded to classify new inputs. The learning process outputs metric values that is used below to find a model with best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best fold ,was 1 with F1 of 0.022838499184339316 at epoch 1\n"
     ]
    }
   ],
   "source": [
    "best_fold = f1_per_fold.index(sorted(f1_per_fold, key=lambda x:x[0], reverse=True)[0]) + 1\n",
    "best_f1, best_epoch,_,_ = f1_per_fold[best_fold-1]\n",
    "print(f'The best fold ,was {best_fold} with F1 of {best_f1} at epoch {best_epoch}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
